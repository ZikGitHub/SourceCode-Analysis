{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Aware Source Code Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/ZikGitHub/Medical-Chatbot-GenerativeAI.git\", \"test_repo\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                       glob=\"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser=LanguageParser(language=\"python\", parser_threshold=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': 'python'}, page_content='from flask import Flask, render_template, request, jsonify\\nfrom src.helper import download_embedding\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.llms import OpenAI'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': 'python'}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name = \"Generative AI Project\",\\n    version = \"0.0.0\",\\n    author = \"Zishan Khan\",\\n    author_email = \"zishankhan@pm.me\",\\n    packages = find_packages(),\\n    install_requires = []\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': 'python'}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\"%(asctime)s [%(levelname)s] %(message)s\"\\n)\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \".env\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor file_path in list_of_files:\\n    file_path = Path(file_path)\\n    file_dir, file_name = os.path.split(file_path)\\n\\n    if file_dir != \"\":\\n        os.makedirs(file_dir, exist_ok=True)\\n        logging.info(f\"Creating directory: {file_dir} for file: {file_name}\")\\n\\n    if (not os.path.exists(file_path)) or (os.path.getsize(file_path) == 0):\\n        with open(file_path, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {file_path}\")\\n\\n    else:\\n        logging.info(f\"File already exists: {file_path}\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': 'python'}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(\\n        data,\\n        glob=\"*.pdf\",\\n        loader_cls=PyPDFLoader,\\n    )\\n    documents = loader.load()\\n    return documents\\n\\n\\ndef text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(\\n        chunk_size=500,\\n        chunk_overlap=20,\\n    )\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n\\ndef download_embedding():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': 'python'}, page_content='system_prompt = (\\n    \"You are an assistant for a question answering system. \"\\n    \"Use the following pieces of context to answer the user\\'s question.\"\\n    \"If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\"\\n    \"Answer correctly.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n    \"\\\\n\\\\n\"\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': 'python'}, page_content='from src.helper import load_pdf_file, text_split, download_embedding\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nos.environ[\"PINECONE_API_KEY\"] = os.environ.get(\"PINE_CONE_API_KEY\")\\n\\nextracted_data = load_pdf_file(data=\"data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_embedding()\\n\\napi_key = os.environ.get(\"PINE_CONE_API_KEY\")\\npc = Pinecone(api_key= api_key)\\n\\nindex = \"testbot\"\\n\\n# pc.create_index(\\n#     name = index,\\n#     dimension=384,\\n#     metric=\"cosine\",\\n#     spec=ServerlessSpec(\\n#         cloud=\"aws\",\\n#         region=\"us-east-1\"\\n#     )\\n# )\\n\\n# docsearch = PineconeVectorStore.from_documents(\\n#     documents=text_chunks,\\n#     index_name=index,\\n#     embedding=embeddings,\\n# )\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': 'python'}, page_content='')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_repo\\\\app.py', 'language': 'python'}, page_content='from flask import Flask, render_template, request, jsonify\\nfrom src.helper import download_embedding\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.llms import OpenAI')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter =  RecursiveCharacterTextSplitter.from_language(language=\"python\", chunk_size=500, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': 'python'}, page_content='from flask import Flask, render_template, request, jsonify\\nfrom src.helper import download_embedding\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.llms import OpenAI'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': 'python'}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name = \"Generative AI Project\",\\n    version = \"0.0.0\",\\n    author = \"Zishan Khan\",\\n    author_email = \"zishankhan@pm.me\",\\n    packages = find_packages(),\\n    install_requires = []\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': 'python'}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\"%(asctime)s [%(levelname)s] %(message)s\"\\n)\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \".env\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor file_path in list_of_files:\\n    file_path = Path(file_path)\\n    file_dir, file_name = os.path.split(file_path)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': 'python'}, page_content='if file_dir != \"\":\\n        os.makedirs(file_dir, exist_ok=True)\\n        logging.info(f\"Creating directory: {file_dir} for file: {file_name}\")\\n\\n    if (not os.path.exists(file_path)) or (os.path.getsize(file_path) == 0):\\n        with open(file_path, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {file_path}\")\\n\\n    else:\\n        logging.info(f\"File already exists: {file_path}\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': 'python'}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(\\n        data,\\n        glob=\"*.pdf\",\\n        loader_cls=PyPDFLoader,\\n    )\\n    documents = loader.load()\\n    return documents'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': 'python'}, page_content='def text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(\\n        chunk_size=500,\\n        chunk_overlap=20,\\n    )\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n\\ndef download_embedding():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': 'python'}, page_content='system_prompt = (\\n    \"You are an assistant for a question answering system. \"\\n    \"Use the following pieces of context to answer the user\\'s question.\"\\n    \"If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\"\\n    \"Answer correctly.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n    \"\\\\n\\\\n\"\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': 'python'}, page_content='from src.helper import load_pdf_file, text_split, download_embedding\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nos.environ[\"PINECONE_API_KEY\"] = os.environ.get(\"PINE_CONE_API_KEY\")\\n\\nextracted_data = load_pdf_file(data=\"data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_embedding()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': 'python'}, page_content='api_key = os.environ.get(\"PINE_CONE_API_KEY\")\\npc = Pinecone(api_key= api_key)\\n\\nindex = \"testbot\"\\n\\n# pc.create_index(\\n#     name = index,\\n#     dimension=384,\\n#     metric=\"cosine\",\\n#     spec=ServerlessSpec(\\n#         cloud=\"aws\",\\n#         region=\"us-east-1\"\\n#     )\\n# )\\n\\n# docsearch = PineconeVectorStore.from_documents(\\n#     documents=text_chunks,\\n#     index_name=index,\\n#     embedding=embeddings,\\n# )')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = documents_splitter.split_documents(documents)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"What is a heart disease?\")\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (0.4.4)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (2.32.3)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.9 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (1.10.21)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (0.7.2)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (0.99.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (3.6.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.1.24)\n",
      "Requirement already satisfied: packaging in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from pulsar-client>=3.1.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from requests>=2.28->chromadb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from requests>=2.28->chromadb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from requests>=2.28->chromadb) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.2)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (4.8.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.1)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(texts, embedding = embeddings, persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zisha\\AppData\\Local\\Temp\\ipykernel_8232\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(temperature=0.4, max_tokens=600, model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zisha\\AppData\\Local\\Temp\\ipykernel_8232\\2101274949.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectordb.as_retriever(search_type='mmr', search_kwargs={'k': 8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is load_pdf_file function?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 9, updating n_results = 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
