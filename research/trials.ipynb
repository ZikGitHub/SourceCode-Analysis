{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Aware Source Code Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\pydantic\\v1\\validators.py:767: UserWarning: Mixing V1 and V2 models is not supported. `BaseLanguageModel` is a V2 model.\n",
      "  warn(f'Mixing V1 and V2 models is not supported. `{type_.__name__}` is a V2 model.', UserWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "no validator found for <class 'langchain_core.language_models.base.BaseLanguageModel'>, see `arbitrary_types_allowed` in Config",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_ollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationSummaryMemory\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationalRetrievalChain\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\langchain\\memory\\__init__.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreadonly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadOnlySharedMemory\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimple\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleMemory\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationSummaryMemory\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary_buffer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationSummaryBufferMemory\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_buffer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationTokenBufferMemory\n",
      "File \u001b[1;32mc:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\langchain\\memory\\summary.py:26\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_memory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseChatMemory\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SUMMARY_PROMPT\n\u001b[0;32m     18\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\n\u001b[0;32m     19\u001b[0m     since\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.12\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefer here for how to incorporate summaries of conversation history: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     ),\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSummarizerMixin\u001b[39;00m(BaseModel):\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mixin for summarizer.\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     human_prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\pydantic\\v1\\main.py:197\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    190\u001b[0m         is_untouched(value)\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m ann_type \u001b[38;5;241m!=\u001b[39m PyObject\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m         )\n\u001b[0;32m    195\u001b[0m     ):\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     fields[ann_name] \u001b[38;5;241m=\u001b[39m \u001b[43mModelField\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mann_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mann_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ann_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39munderscore_attrs_are_private:\n\u001b[0;32m    205\u001b[0m     private_attributes[ann_name] \u001b[38;5;241m=\u001b[39m PrivateAttr()\n",
      "File \u001b[1;32mc:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\pydantic\\v1\\fields.py:504\u001b[0m, in \u001b[0;36mModelField.infer\u001b[1;34m(cls, name, value, annotation, class_validators, config)\u001b[0m\n\u001b[0;32m    501\u001b[0m     required \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    502\u001b[0m annotation \u001b[38;5;241m=\u001b[39m get_annotation_from_field_info(annotation, field_info, name, config\u001b[38;5;241m.\u001b[39mvalidate_assignment)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_validators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_validators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\pydantic\\v1\\fields.py:434\u001b[0m, in \u001b[0;36mModelField.__init__\u001b[1;34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m SHAPE_SINGLETON\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mprepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\pydantic\\v1\\fields.py:555\u001b[0m, in \u001b[0;36mModelField.prepare\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;129;01mis\u001b[39;00m Undefined \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulate_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\pydantic\\v1\\fields.py:829\u001b[0m, in \u001b[0;36mModelField.populate_validators\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_fields \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m SHAPE_GENERIC:\n\u001b[0;32m    826\u001b[0m     get_validators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__get_validators__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    827\u001b[0m     v_funcs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    828\u001b[0m         \u001b[38;5;241m*\u001b[39m[v\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m class_validators_ \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39meach_item \u001b[38;5;129;01mand\u001b[39;00m v\u001b[38;5;241m.\u001b[39mpre],\n\u001b[1;32m--> 829\u001b[0m         \u001b[38;5;241m*\u001b[39m(get_validators() \u001b[38;5;28;01mif\u001b[39;00m get_validators \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfind_validators\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;241m*\u001b[39m[v\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m class_validators_ \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39meach_item \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mpre],\n\u001b[0;32m    831\u001b[0m     )\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidators \u001b[38;5;241m=\u001b[39m prep_validators(v_funcs)\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_validators \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\pydantic\\v1\\validators.py:768\u001b[0m, in \u001b[0;36mfind_validators\u001b[1;34m(type_, config)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(type_, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__pydantic_core_schema__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    767\u001b[0m     warn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMixing V1 and V2 models is not supported. `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is a V2 model.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[1;32m--> 768\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno validator found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, see `arbitrary_types_allowed` in Config\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: no validator found for <class 'langchain_core.language_models.base.BaseLanguageModel'>, see `arbitrary_types_allowed` in Config"
     ]
    }
   ],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/ZikGitHub/Medical-Chatbot-GenerativeAI.git\", \"test_repo\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                       glob=\"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser=LanguageParser(language=\"python\", parser_threshold=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': 'python'}, page_content='from flask import Flask, render_template, request, jsonify\\nfrom src.helper import download_embedding\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.llms import OpenAI'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': 'python'}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name = \"Generative AI Project\",\\n    version = \"0.0.0\",\\n    author = \"Zishan Khan\",\\n    author_email = \"zishankhan@pm.me\",\\n    packages = find_packages(),\\n    install_requires = []\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': 'python'}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\"%(asctime)s [%(levelname)s] %(message)s\"\\n)\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \".env\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor file_path in list_of_files:\\n    file_path = Path(file_path)\\n    file_dir, file_name = os.path.split(file_path)\\n\\n    if file_dir != \"\":\\n        os.makedirs(file_dir, exist_ok=True)\\n        logging.info(f\"Creating directory: {file_dir} for file: {file_name}\")\\n\\n    if (not os.path.exists(file_path)) or (os.path.getsize(file_path) == 0):\\n        with open(file_path, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {file_path}\")\\n\\n    else:\\n        logging.info(f\"File already exists: {file_path}\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': 'python'}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(\\n        data,\\n        glob=\"*.pdf\",\\n        loader_cls=PyPDFLoader,\\n    )\\n    documents = loader.load()\\n    return documents\\n\\n\\ndef text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(\\n        chunk_size=500,\\n        chunk_overlap=20,\\n    )\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n\\ndef download_embedding():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': 'python'}, page_content='system_prompt = (\\n    \"You are an assistant for a question answering system. \"\\n    \"Use the following pieces of context to answer the user\\'s question.\"\\n    \"If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\"\\n    \"Answer correctly.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n    \"\\\\n\\\\n\"\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': 'python'}, page_content='from src.helper import load_pdf_file, text_split, download_embedding\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nos.environ[\"PINECONE_API_KEY\"] = os.environ.get(\"PINE_CONE_API_KEY\")\\n\\nextracted_data = load_pdf_file(data=\"data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_embedding()\\n\\napi_key = os.environ.get(\"PINE_CONE_API_KEY\")\\npc = Pinecone(api_key= api_key)\\n\\nindex = \"testbot\"\\n\\n# pc.create_index(\\n#     name = index,\\n#     dimension=384,\\n#     metric=\"cosine\",\\n#     spec=ServerlessSpec(\\n#         cloud=\"aws\",\\n#         region=\"us-east-1\"\\n#     )\\n# )\\n\\n# docsearch = PineconeVectorStore.from_documents(\\n#     documents=text_chunks,\\n#     index_name=index,\\n#     embedding=embeddings,\\n# )\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': 'python'}, page_content='')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_repo\\\\app.py', 'language': 'python'}, page_content='from flask import Flask, render_template, request, jsonify\\nfrom src.helper import download_embedding\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.llms import OpenAI')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter =  RecursiveCharacterTextSplitter.from_language(language=\"python\", chunk_size=500, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': 'python'}, page_content='from flask import Flask, render_template, request, jsonify\\nfrom src.helper import download_embedding\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain.llms import OpenAI'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': 'python'}, page_content='from setuptools import find_packages, setup\\n\\nsetup(\\n    name = \"Generative AI Project\",\\n    version = \"0.0.0\",\\n    author = \"Zishan Khan\",\\n    author_email = \"zishankhan@pm.me\",\\n    packages = find_packages(),\\n    install_requires = []\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': 'python'}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\"%(asctime)s [%(levelname)s] %(message)s\"\\n)\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \".env\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor file_path in list_of_files:\\n    file_path = Path(file_path)\\n    file_dir, file_name = os.path.split(file_path)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': 'python'}, page_content='if file_dir != \"\":\\n        os.makedirs(file_dir, exist_ok=True)\\n        logging.info(f\"Creating directory: {file_dir} for file: {file_name}\")\\n\\n    if (not os.path.exists(file_path)) or (os.path.getsize(file_path) == 0):\\n        with open(file_path, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {file_path}\")\\n\\n    else:\\n        logging.info(f\"File already exists: {file_path}\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': 'python'}, page_content='from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(\\n        data,\\n        glob=\"*.pdf\",\\n        loader_cls=PyPDFLoader,\\n    )\\n    documents = loader.load()\\n    return documents'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': 'python'}, page_content='def text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(\\n        chunk_size=500,\\n        chunk_overlap=20,\\n    )\\n    text_chunks = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n\\ndef download_embedding():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': 'python'}, page_content='system_prompt = (\\n    \"You are an assistant for a question answering system. \"\\n    \"Use the following pieces of context to answer the user\\'s question.\"\\n    \"If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\"\\n    \"Answer correctly.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n    \"\\\\n\\\\n\"\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': 'python'}, page_content='from src.helper import load_pdf_file, text_split, download_embedding\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nos.environ[\"PINECONE_API_KEY\"] = os.environ.get(\"PINE_CONE_API_KEY\")\\n\\nextracted_data = load_pdf_file(data=\"data/\")\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_embedding()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': 'python'}, page_content='api_key = os.environ.get(\"PINE_CONE_API_KEY\")\\npc = Pinecone(api_key= api_key)\\n\\nindex = \"testbot\"\\n\\n# pc.create_index(\\n#     name = index,\\n#     dimension=384,\\n#     metric=\"cosine\",\\n#     spec=ServerlessSpec(\\n#         cloud=\"aws\",\\n#         region=\"us-east-1\"\\n#     )\\n# )\\n\\n# docsearch = PineconeVectorStore.from_documents(\\n#     documents=text_chunks,\\n#     index_name=index,\\n#     embedding=embeddings,\\n# )')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = documents_splitter.split_documents(documents)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"What is a heart disease?\")\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (0.4.4)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (2.32.3)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.9 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (1.10.21)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (0.7.2)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (0.99.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (3.6.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.1.24)\n",
      "Requirement already satisfied: packaging in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from pulsar-client>=3.1.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from requests>=2.28->chromadb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from requests>=2.28->chromadb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from requests>=2.28->chromadb) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.2)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (4.8.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.1)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\zisha\\.conda\\envs\\env-bappysource\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m----> 2\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mtexts\u001b[49m, embedding \u001b[38;5;241m=\u001b[39m embeddings, persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(texts, embedding = embeddings, persist_directory=\"db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
